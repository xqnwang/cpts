---
title: "Weighted quantile estimators"
format: 
  pdf:
    number-sections: true
    link-citations: true
bibliography: references.bib
---

```{r}
#| label: setup
#| echo: FALSE
#| message: false
#| warning: false

library(ggplot2)
library(dplyr)
library(rlang)
library(patchwork)

theme_set(ggdist::theme_ggdist())
```

# The method commonly used in CP papers

```{r}
#| label: cp
#| eval: false
weighted.quantile <- function(x, p, w = NULL, sorted = FALSE) {
  if (is.null(w)) w <- rep(1, length(x))
  if (!sorted) { o <- order(x); x <- x[o]; w <- w[o] }
  i <- which(cumsum(w/sum(w)) >= p)
  if (length(i) == 0) return(Inf) # Can happen with infinite weights
  else return(x[min(i)])
}
```

It corresponds to Type 1[@hyndman1996]. The mathematical inverse $\tilde{F}^{-1}(p)$ of the CDF is the smallest $x$ such that $F(x) \geq p$.

# @akinshin2023

## Desired properties of the weighted quantile estimators

-   **R1:** consistency with existing quantile estimators.

    $$
    \mathrm{Q}^*(\mathbf{x},\mathbf{w}, p)=\mathrm{Q}(\mathbf{x}, p), \text{ where } \mathbf{w}=\{1,1, \ldots, 1\}.
    $$

-   **R2:** zero weight support.

    $$
    \mathrm{Q}^*\left(\left\{x_1, x_2, \ldots, x_{n-1}, x_n\right\},\left\{w_1, w_2, \ldots, w_{n-1}, 0\right\}, p\right)=\mathrm{Q}^*\left(\left\{x_1, x_2, \ldots, x_{n-1}\right\},\left\{w_1, w_2, \ldots, w_{n-1}\right\}, p\right).
    $$

-   **R3:** stability (continuity of the quantile estimations with respect to the weight coefficient).

    $$
    \lim _{\varepsilon_i \rightarrow 0} Q^*\left(\left\{x_1, x_2, \ldots, x_n\right\},\left\{w_1+\varepsilon_1, w_2+\varepsilon_2, \ldots, w_n+\varepsilon_n\right\}, p\right) \rightarrow \mathrm{Q}^*\left(\left\{x_1, x_2, \ldots, x_n\right\},\left\{w_1, w_2, \ldots, w_n\right\}, p\right).
    $$

## Some issues

1.  Some simple R package implementations of weighted quantiles show **violations of R3**.

    ```{r}
    #| label: R_package_examples
    #| eval: false
    x <- c(0, 1, 100)
    wA <- c(1, 0.00000, 1)
    wB <- c(1, 0.00001, 1)
    message(quantile(c(0, 100), 0.5))
    ## 50
    message(modi::weighted.quantile(x, wA, 0.5), " | ",
            modi::weighted.quantile(x, wB, 0.5))
    ## 100 | 1
    message(laeken::weightedQuantile(x, wA, 0.5), " | ",
            laeken::weightedQuantile(x, wB, 0.5))
    ## 100 | 1
    message(MetricsWeighted::weighted_quantile(x, wA, 0.5), " | ",
            MetricsWeighted::weighted_quantile(x, wB, 0.5))
    ## 100 | 1
    message(spatstat.geom::weighted.quantile(x, wA, 0.5), " | ",
            spatstat.geom::weighted.quantile(x, wB, 0.5))
    ## 1 | 0.499999999994449
    message(matrixStats::weightedMedian(x, wA), " | ",
            matrixStats::weightedMedian(x, wB))
    ## 50 | 1.00000000000003
    ```

2.  In order to satisfy R2, the sample size needs adjustments.

    ```{r}
    #| label: R_package_examples1
    #| eval: false

    non_zero <- wA != 0
    message(modi::weighted.quantile(x, wA, 0.5), " | ",
            modi::weighted.quantile(x[non_zero], wA[non_zero], 0.5))
    ## 100 | 50
    message(laeken::weightedQuantile(x, wA, 0.5), " | ",
            laeken::weightedQuantile(x[non_zero], wA[non_zero], 0.5))
    ## 100 | 100
    message(MetricsWeighted::weighted_quantile(x, wA, 0.5), " | ",
            MetricsWeighted::weighted_quantile(x[non_zero], wA[non_zero], 0.5))
    ## 100 | 100
    message(spatstat.geom::weighted.quantile(x, wA, 0.5), " | ",
            spatstat.geom::weighted.quantile(x[non_zero], wA[non_zero], 0.5))
    ## 1 | 0
    message(matrixStats::weightedMedian(x, wA), " | ",
            matrixStats::weightedMedian(x[non_zero], wA[non_zero]))
    ## 50 | 50
    ```

    Consider using **Kish's effective sample size** given by

    $$
    n^*(\mathbf{w})=\frac{\left(\sum_{i=1}^n w_i\right)^2}{\sum_{i=1}^n w_i^2}=\frac{1}{\sum_{i=1}^n \bar{w}_i^2},
    $$

    where $\mathbf{w}$ is the vector of non-negative weight coefficients, $\bar{\mathbf{w}}$ is the vector of normalized weights where $\bar{w}_i=\frac{w_i}{\sum_{i=1}^{n}w_i}$. So $n^*(\{1,1,1\})=3$ and $n^*(\{1,1,1,0,0\})=3$.

## Method 1: Weighted Harrell--Davis quantile estimator

-   **A linear combination of all order statistics.**

-   **Pros:** provide higher statistical efficiency in the cases of light-tailed distributions.

-   **Cons:** not robust (its breakdown point is zero).

    The weighted Harrell--Davis quantile estimator is defined as

    $$
    \mathrm{Q}_{\mathrm{HD}}^*(\mathbf{x}, \mathbf{w}, p)=\sum_{i=1}^n W_{\mathrm{HD}, i}^* \cdot x_{(i)}, \quad W_{\mathrm{HD}, i}^*=I_{t_i^*}\left(\alpha^*, \beta^*\right)-I_{t_{i-1}^*}\left(\alpha^*, \beta^*\right),
    $$

    where $I_{t_i^*}\left(\alpha^*, \beta^*\right)$ is the CDF of the beta distribution $\text{Beta}(\alpha^*, \beta^*)$, $\alpha^*=\left(n^*+1\right) p$, $\beta^*=\left(n^*+1\right)(1-p)$, $t_i^*=s_i(\bar{\mathbf{w}})$ is the partial sum of normalized weight coefficients.

    $W_{\mathrm{HD}, i}^*$ **can be considered as probabilities of observing the target quantile at the given position.** See details from Examples 7 and 8 in the paper.

    For all $p\in(0;1)$, $\mathrm{Q}_{\mathrm{HD}}^*(\mathbf{x}, \mathbf{w}, p)$ satisfies R1, R2, and R3.

## Method 2: Weighted trimmed Harrell--Davis quantile estimator

-   **A trimmed modification** of the HD method.

-   **Idea:** since most of the linear coefficients $W_{\mathrm{HD}, i}^*$ are pretty small, they do not have a noticeable impact on efficiency, but they significantly reduce the breakdown point.

-   **Pros:** allow customizing trade-off between robustness and efficiency.

-   **Cons:** use the rule of thumb to decide $[L^*;R^*]$.

    For $p \in (0;1)$, the weighted trimmed Harrell--Davis quantile estimator based on the beta distribution highest density interval $[L^*; R^*]$ of the given size $D^*$ (the rule of thumb: $D^*=1/\sqrt{n^*}$) is defined as

    $$
    \mathrm{Q}_{\mathrm{THD}}^*(\mathbf{x}, \mathbf{w}, p)=\sum_{i=1}^n W_{\mathrm{THD}, i}^* \cdot x_{(i)}, \quad W_{\mathrm{THD}, i}^*=F_{\mathrm{THD}}^*\left(t_i^*\right)-F_{\mathrm{THD}}^*\left(t_{i-1}^*\right), \quad t_i^*=s_i(\overline{\mathbf{w}}),
    $$

    where

    $$
    F_{\mathrm{THD}}^*(t)= \begin{cases}0 & \text { for } t<L^*, \\\left(I_t\left(\alpha^*, \beta^*\right)-I_{L^*}\left(\alpha^*, \beta^*\right)\right) /\left(I_{R^*}\left(\alpha^*, \beta^*\right)-I_{L^*}\left(\alpha^*, \beta^*\right)\right) & \text { for } L^* \leq t \leq R^*, \\1 & \text { for } R^*<t .\end{cases}
    $$

## Method 3: Weighted traditional quantile estimators

-   Implement the framework of the Harrell--Davis quantile estimator to @hyndman1996 in which Types 1-9 are considered.

    -   Types 1--3 have discontinuities, so the corresponding estimators fail to satisfy R3.

    -   Here only Types 4-9 (using a linear combination of **two order statistics**) are considered.

-   **Pros:** robust, not so efficient.

    $$
    \begin{array}{lll}
    \multicolumn{3}{c}{\text{Table: The Hyndman \& Fan (1996) taxonomy of quantile estimators.}} \\
    \hline 
    \text { Type } & \mathrm{h} & \text { Equation } \\\hline 1 & n p & x_{(\lceil h\rceil)} \\
    2 & n p+1 / 2 & \left(x_{(\lceil h-1 / 2\rceil)}+x_{(\lceil h+1 / 2\rceil)}\right) / 2 \\
    3 & n p & x_{(\lfloor h\rceil)} \\
    4 & n p & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    5 & n p+1 / 2 & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    6 & (n+1) p & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    7 & (n-1) p+1 & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    8 & (n+1 / 3) p+1 / 3 & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    9 & (n+1 / 4) p+3 / 8 & x_{(\lfloor h\rfloor)}+(h-\lfloor h\rfloor)\left(x_{(\lceil h\rceil)}-x_{(\lfloor h\rfloor)}\right) \\
    \hline
    \end{array}
    $$

    The weighted case of these quantile estimators can be rewritten in a form that matches the definition of $\mathrm{Q}_{\mathrm{THD}}^*$ given by

    $$
    Q_k^*(\mathbf{x}, \mathbf{w}, p)=\sum_{i=1}^{n} W_{F_k^*, i}^* \cdot x_{(i)}, \quad W_{F_k^*, i}^*=F_k^*\left(t_i^*\right)-F_k^*\left(t_{i-1}^*\right), \quad t_i^*=s_i(\bar{\mathbf{w}}),
    $$

    where

    $$
    F_k^*(t)=\left\{\begin{array}{llrr}0 & \text { for } & t<(h^*-1)/n^*, & \\t n^*-h^*+1 & \text { for } & \left(h^*-1\right) / n^* \leq t \leq h^*/n^*, \\1 & \text { for } & t > h^* / n^*, \end{array}\right.
    $$

    where $h^*$ is calculated based on $p^*$ rather than $p$. So, instead of Beta distribution, uniform distribution is suggested in the paper to define the required PDF and CDF.

## Summary

-   The methods tend to be too smooth as they tend to use more order statistics (more than two) for linear combination to get the weighted quantile.

-   Regarding conformal prediction, we normally calculate $\mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \tilde{w}_i \cdot \delta_{R_i}+\tilde{w}_{n+1} \cdot \delta_{+\infty}\right)$ in which the maximum is Inf, then the resulting weighted quantile tends to return Inf as well.

-   We also tried to remove Inf from the calculation of the required quantile, the average coverage is still biased compared to nominal coverage, which is the same case when employing the weighted quantile method commonly used in conformal prediction papers.

# The survey package

[Quantile rules in the survey R package by Thomas Lumley.](https://cran.r-project.org/web/packages/survey/vignettes/qrule.pdf)

In the `svyquantile` function of the **survey** R package, there is an argument named qrule in which we can choose the rule/function used to define the quantiles.

## Discontinuous quantiles

$$
\hat{F}(x)=\frac{\sum_i\left\{x_i \leq x\right\} w_{(i)}}{\sum_i w_{(i)}}
$$

```{r}
#| label: qs_fun
#| eval: false

last <- function(x) {
  if (any(x)) max(which(x)) else 1
}

qs <- function(x, w, p){
  n <- length(x)
  ii <- order(x)
  x <- x[ii]
  cumw <- cumsum(w[ii])
  
  pos <- last(cumw <= p*sum(w))
  posnext <- if (pos == n) pos else pos+1
  
  list(qlow = x[pos], qup = x[posnext],
       ilow = pos, iup = posnext,
       wlow = p - cumw[pos]/sum(w), wup = cumw[posnext]/sum(w) - p)
}
```

**Type1:** the smallest $x$ such that $F(x) \geq p$.

```{r}
#| label: qrule_1
#| eval: false

qdata <- qs(x, w, p)
if (qdata$wlow == 0) qdata$qlow else qdata$qup
```

**Type 2:** $q_{\text {low }}=\hat{F}^{-1}(p)$ if $\hat{F}\left(q_{\text {low }}\right)=p$ and otherwise is the the average of $\hat{F}^{-1}(p)$ and the next higher observation.

```{r}
#| label: qrule_2
#| eval: false

qdata <- qs(x, w, p)
if (qdata$wlow == 0) (qdata$qlow + qdata$qup)/2 else qdata$qup
```

**Type 3:** whichever of $\hat{F}^{-1}(p)$ and the next higher observation is at an even-numbered position when the distinct data values are sorted.

```{r}
#| label: qrule_3
#| eval: false

w <- rowsum(w, x)
x <- sort(unique(x))
qdata <- qs(x, w, p)
if ((qdata$wlow == 0) && (qdata$ilow %% 2 == 0)) qdata$qlow else qdata$qup
```

**Some errors?**

```{r}
#| label: qrule_example_1-3

# Example data
source("../R/qrule.R") # error?

x <- c(1, 3, 4, 7, 9, 10.5)
w <- c(0.1, 0.1, 0.2, 0.3, 0.1, 0.2)
ps <- seq(0.05, 0.99, 0.05)

cbind(p = ps,
      Type1 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 1)),
      Type2 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 2)),
      Type3 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 3)))
```

## Continuous quantiles

For the plotting position, it is redefined in terms of the cumulative weights $W_{(k)} = \sum_{i \leq k} w_{(i)}$, the total weight $W_{(n)}$, and the weight $w_{(k)}$ on the $k$th observation, as shown in the table below (**typos fixed**).

The following weighted quantiles reduce to unweighted cases for equal weights settings.

$$
\begin{array}{lll}
\hline 
\text { Method } & \text { Hyndman \& Fan } & \text { Weighted quantiles} \\
\hline 
\text { Type 4 } & p_k=k / n & p_k=W_{(k)} / W_{(n)} \\
\text { Type 5 } & p_k=(k-1 / 2) / n & p_k=\left(W_{(k)}-w_{(k)} / 2\right) / W_{(n)} \\
\text { Type 6 } & p_k=k /(n+1) & p_k=W_{(k)} /\left(W_{(n)}+w_{(n)}\right) \\
\text { Type 7 } & p_k=(k-1) /(n-1) & p_k=W_{(k-1)} / W_{(n-1)} \\
\text { Type 8 } & p_k=(k-1 / 3) /(n+1 / 3) & p_k=\left(W_{(k)}-w_{(k)} / 3\right) /\left(W_{(n)}+w_{(n)} / 3\right) \\
\text { Type 9 } & p_k=(k-3 / 8) /(n+1 / 4) & p_k=\left(W_{(k)}-3 w_{(k)} / 8\right) /\left(W_{(n)}+w_{(n)} / 4\right) \\
\hline
\end{array}
$$

```{r}
#| label: qrule_example_4-9

cbind(p = ps,
      Type4 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 4)),
      Type5 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 5)),
      Type6 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 6)),
      Type7 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 7)),
      Type8 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 8)),
      Type9 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 9))) |> 
  round(2)
```

## Shah & Vaish [@shah2006]

Define $w_{(i)}^*=w_{(i)} n / W_{(n)}$, so that $w_{(i)}^*$ sum to the sample size, and $C^* k$ as partial sums of $w_{(k)}^*$.

The estimated CDF is defined by

$$
\hat{F}\left(x_k\right)=\frac{1}{n+1}\left(W_{(k)}^*+1 / 2-w_{(k)} / 2\right).
$$

It is related to **Type 6**.

# Weighted quantiles - Matthew Kay

The `weighted_quantile` function used in the **ggdist** R package and described in the [blog](https://htmlpreview.github.io/?https://github.com/mjskay/uncertainty-examples/blob/master/weighted-quantiles.html).

## Source code

```{r}
#| label: ggdist
#| eval: false

# In the weighted_quantile function
if (1 <= type && type <= 3) {
  # discontinuous quantiles
  switch(type,
         # type 1
         stepfun(F_x, c(x, x[length(x)]), right = TRUE),
         # type 2
         {
           x_over_2 = c(x, x[length(x)])/2
           inverse_cdf_type2_left = stepfun(F_x, x_over_2, right = FALSE)
           inverse_cdf_type2_right = stepfun(F_x, x_over_2, right = TRUE)
           function(x) inverse_cdf_type2_left(x) + inverse_cdf_type2_right(x)
         },
         # type 3
         stepfun(F_x - f_x/2, c(x[[1]], x), right = TRUE)
  )
} else {
  # Continuous quantiles
  p_k = switch(type - 3,
               # type 4
               F_x,
               # type 5
               F_x - f_x/2,
               # type 6
               F_x / (1 + f_x),
               # type 7
               (F_x - f_x) / (1 - f_x),
               # type 8
               (F_x - f_x/3) / (1 + f_x/3),
               # type 9
               (F_x - f_x*3/8) / (1 + f_x/4)
  )
  approxfun(p_k, x, rule = 2, ties = "ordered")
}
```

## Continuous quantiles

Per @hyndman1996, the unweighted quantile function is that of a piecewise linear interpolation between the points $\left[p_k, X_{(k)}\right]$, with specific definitions of $p_k$ for each quantile type depending on its definition of $m$:

$$
p_k = \frac{k - \alpha}{n - \alpha - \beta + 1}\\
m = \alpha + p(1 - \alpha - \beta)
$$

-   The formulas for $p_k$ in the unweighted case are written in terms of $k$ and $n$.
-   The **survey** package obtains $p_k$ in terms of the cumulative weights $W_{(k)} = \sum_{i \leq k} w_{(i)}$, the total weight $W_{(n)}$, and the weight $w_{(k)}$ on the $k$th observation.
-   The **ggdist** package here obtains $p_k$ in terms of the $W_{(k)}$ and $w_{(k)}$.

The following two identities hold in the unweighted case:

$$
n = \frac{1}{w_{(k)}} \text{ and }
k = nW_{(k)} = \frac{W_{(k)}}{w_{(k)}}.
$$

So, we have

$$
p_k = \frac{k - \alpha}{n - \alpha - \beta + 1} = \frac{\frac{W_{(k)}}{w_{(k)}} - \alpha}{\frac{1}{w_{(k)}} - \alpha - \beta + 1} = \frac{W_{(k)} - w_{(k)} \alpha } {1 + w_{(k)} (1 - \alpha - \beta)}.
$$

**It is slightly different from the weighted quantile method in the survey package.**

```{r}
#| label: ggdist_example_4-9

cbind(p = ps,
      Type4 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 4)),
      Type4_ = ggdist::weighted_quantile(x, ps, w, type = 4),
      Type5 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 5)),
      Type5_ = ggdist::weighted_quantile(x, ps, w, type = 5),
      Type6 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 6)),
      Type6_ = ggdist::weighted_quantile(x, ps, w, type = 6),
      Type7 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 7)),
      Type7_ = ggdist::weighted_quantile(x, ps, w, type = 7),
      Type8 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 8)),
      Type8_ = ggdist::weighted_quantile(x, ps, w, type = 8),
      Type9 = sapply(ps, function(p) weighted.quantile.qrule(x, w, p, type = 9)),
      Type9_ = ggdist::weighted_quantile(x, ps, w, type = 9)) |> 
  round(2)
```

## Create flat regions

To satisfy the property that $\hat{Q}_{X,i}(0.5)$ should be equal to the sample median.

1.  Estimate the sample size using Kish's effective sample size.
2.  Construct a modified dataset with replications proportional to weights \-\-- Introduce flat regions.

The weighted quantiles now match the unweighted quantiles much more closely. I think this strategy helps a lot.

# Comparison

## Three requirements proposed in @akinshin2023

```{r}
#| label: comp_R3

x <- c(0, 1, 100)
wE <- c(1, 1, 1)
wA <- c(1, 0, 1); nz <- wA != 0
wB <- c(1, 0.00001, 1)

kess <- function(w) sum(w)^2 / sum(w^2)
p <- 0.9

# R1: consistency with existing quantile estimators
sapply(1:9, function(type){
  c(quantile = quantile(x, probs = p, type),
    survey = weighted.quantile.qrule(x, w = wE, p = p, type),
    ggdist = ggdist::weighted_quantile(x, weights = wE,
                                       probs = p, type),
    ggdist_kess = ggdist::weighted_quantile(x, weights = wE,
                                            probs = p, type, n = kess))
}, simplify = TRUE, USE.NAMES = TRUE) |> t()
## If Kish's effective sample size is not used in ggdist, R1 is violated.

# R2: zero weight support
sapply(1:9, function(type){
  c(quantile = quantile(x[nz], probs = p, type),
    survey = weighted.quantile.qrule(x, w = wA, p = p, type),
    survey_nz = weighted.quantile.qrule(x[nz], w = wA[nz], p = p, type),
    ggdist = ggdist::weighted_quantile(x, weights = wA, probs = p, type),
    ggdist_nz = ggdist::weighted_quantile(x[nz], weights = wA[nz],
                                          probs = p, type),
    ggdist_kess = ggdist::weighted_quantile(x, weights = wA, probs = p, type, n = kess),
    ggdist_nz_kess = ggdist::weighted_quantile(x[nz], weights = wA[nz],
                                               probs = p, type, n = kess))
}, simplify = TRUE, USE.NAMES = TRUE) |> t()

# R3: stability
p <- 0.5
sapply(1:9, function(type){
  c(quantile = quantile(x[nz], probs = p, type),
    survey_wA = weighted.quantile.qrule(x, w = wA, p = p, type),
    survey_wB = weighted.quantile.qrule(x, w = wB, p = p, type),
    ggdist_wA = ggdist::weighted_quantile(x, weights = wA, probs = p, type),
    ggdist_wB = ggdist::weighted_quantile(x, weights = wB, probs = p, type),
    ggdist_wA_kess = ggdist::weighted_quantile(x, weights = wA, probs = p, type, n = kess),
    ggdist_wB_kess = ggdist::weighted_quantile(x, weights = wB, probs = p, type, n = kess))
}, simplify = TRUE, USE.NAMES = TRUE) |> t() |> round(2)
```

The stability property is not satisfied in either the survey or ggdist packages, but it is not common for neighboring ordered statistics (conformity scores) to differ significantly in CP problems.

**Maybe we can try to propose some locally smooth method to address it.**

## Non-weighted VS weighted quantiles

```{r}
#| label: functions in ggdist blog
#| echo: false

Q_hat_color = "#fc8d62"
Q_hat_w_color = "#66c2a5"
Q_hat_g_color = "#bf71bf"

continuous_quantile_plot = function(type, X = NULL, w = NULL) {
  X = X %||% get("X", parent.frame())
  w = w %||% get("w", parent.frame())
  W = cumsum(w)
  
  (
    ggplot() +
      continuous_quantile_bounds(X_star, w_star) +
      inverse_cdf_plot(X_star, name = "X*", type = type) + 
      labs(subtitle = paste("Type =", type, ": unweighted")) +
      geom_function(fun = \(p) quantile(X_star, p, type = type), color = Q_hat_color, linewidth = 1, n = 1000) 
  ) + (
    ggplot() +
      continuous_quantile_bounds(X, w) +
      inverse_cdf_plot(X, w, type = type) + 
      labs(subtitle = paste("Type =", type, ": survey")) +
      geom_function(fun = \(p) quantile(X_star, p, type = type), color = Q_hat_color, linewidth = 1, n = 1000, alpha = 0.65, linetype = "11") + 
      geom_function(fun = \(p) weighted.quantile.qrule(X, w = w, p = p, type), color = Q_hat_w_color, linewidth = 1, n = 1000)
  ) + (
    ggplot() +
      continuous_quantile_bounds(X, w) +
      inverse_cdf_plot(X, w, type = type) + 
      labs(subtitle = paste("Type =", type, ": ggdist")) +
      geom_function(fun = \(p) quantile(X_star, p, type = type), color = Q_hat_color, linewidth = 1, n = 1000, alpha = 0.65, linetype = "11") + 
      geom_function(fun = \(p) ggdist::weighted_quantile(X, w, probs = p, type = type), color = Q_hat_g_color, linewidth = 1, n = 1000)
  )
}

continuous_quantile_bounds = function(X, w) {
  W = cumsum(w)
  upper_bound = stepfun(W, c(X, X[length(X)]))
  lower_bound = stepfun(W, c(X[1], X))
  p = sort(unique(c(W, W - .Machine$double.eps, W - w)))

  geom_ribbon(aes(c(0, W), ymin = c(X[1], X), ymax = c(X, X[length(X)])), fill = "gray95")
}

inverse_cdf_plot = function(X, w = NULL, name = "X", type = "i") {
  index = if (is.null(w)) paste0("X,", type) else paste0("X,w,", type)
  w = w %||% rep(1/length(X), length(X))
  W = cumsum(w)
  min_x = floor(min(X))
  max_x = ceiling(max(X))

  list(
    geom_step(aes(c(0, W), c(X[[1]], X)), direction = "vh", color = "gray65"),
    geom_point(aes(W, X), color = "gray65", size = 2),
    xlim(0, 1),
    scale_y_continuous(limits = c(min_x, max_x), breaks = seq(min_x, max_x, by = 1)),
    labs(
      x = expression(paste("probability: ", italic(p))),
      y = bquote(paste("quantile: ", italic(hat(Q)[.(index)])(italic(p))))
    )
  )
}
```

```{r}
#| label: non-weighted vs weighted
#| results: hold
#| warning: false
#| message: false

X_star <- c(1,2,3,3,3,3,5,9,10.5)
w_star <- rep(1, length(X_star)) / length(X_star)
W_star <- cumsum(w_star)

X <- unique(X_star)
w <- as.vector(prop.table(table(X_star)))
W <- cumsum(w)
tibble(X, w, W)
```

```{r}
#| label: non-weighted vs weighted results
#| echo: false
#| results: hold
#| warning: false
#| message: false
#| fig-width: 7
#| fig-height: 3

continuous_quantile_plot(4, X, w)
continuous_quantile_plot(5, X, w)
continuous_quantile_plot(6, X, w)
continuous_quantile_plot(7, X, w)
continuous_quantile_plot(8, X, w)
continuous_quantile_plot(9, X, w)
```

The **survey** and **ggdist** packages use different strategies for defining $p_k$ for continuous quantiles (Types 4-9), the quantile outputs are slightly different, but still always within the reasonable bounds.

**Which strategy makes more theoretical sense? We should explore this further.**

## Impact of Inf

```{r}
#| label: inf
#| results: hold

n <- 500
x <- c(rnorm(n, 0, 1), Inf)
x_order <- order(x)
x <- x[x_order]

w <- c(0.99^(n+1-((1:n))), 1)
w <- w/sum(w)
W <- cumsum(w)

# Data example
tibble(x, w, W) |> tail(10)

# 90% weighted quantile for a set including Inf
sapply(1:9, function(type){
  c(survey = weighted.quantile.qrule(x, w = w, p = 0.9, type),
    ggdist = ggdist::weighted_quantile(x, weights = w, probs = 0.9, type),
    ggdist_kess = ggdist::weighted_quantile(x, weights = w, probs = 0.9, type, n = kess))
}, simplify = TRUE, USE.NAMES = TRUE) |> t()

# Harrell-Davis quantile estimator & Type7 in Akinshin (2023)
paste(cNORM::weighted.quantile.harrell.davis(x, 0.9, w), "|",
      cNORM::weighted.quantile.type7(x, 0.9, w))
```

# Summary

-   The `weighted_quantile` function in the **ggdist** R package seems to be a good choice in the context of conformal prediction.

-   To satisfy the property that $\hat{Q}_{X,i}(0.5)$ is equal to the sample median, we suggest using **Kish's effective sample size** to create flat regions and make the output quantiles match the unweighted quantiles more closely. Using Kish's effective sample size is more likely to return quantile estimator satisfying the R1 consistency property.

    ```{r}
    #| label: kess
    #| eval: false

    # Function to calculate Kish's effective sample size
    kess <- function(w) sum(w)^2 / sum(w^2)
    ```

-   **Type 8** may be a good choice as it gives median-unbiased estimates in the equal weight settings, even if all types may give the same result for large sample size.

# References

::: {#refs}
:::
