---
title: "Conformal prediction and its extensions"
format: 
  pdf:
    number-sections: true
    link-citations: true
editor: visual
bibliography: references.bib
---

```{r}
#| label: load
#| cache: false
#| echo: false
#| message: false
#| warning: false
# Load all required packages

library(forecast)
library(zoo)
library(tidyr)
library(reshape2)
library(ggplot2)
library(ggpubr)
```

# CP: Conformal prediction

Methods for distribution-free prediction.

**Assumption: exchangeability**

-   The **data** $Z_i = (X_i, Y_i)$ are assumed to be exchangeable (for example, i.i.d.).
    -   Definition [@vovk2005; @shafer2008]. Suppose that for any collection of $N$ values, the $N!$ different orderings are equally likely. Then we say that $Z_1, \ldots, Z_N$ are exchangeable. The exchangeability assumption is slightly weaker than the i.i.d. assumption.
-   The **algorithm** which maps data to a fitted model $\widehat{\mu}: \mathcal{X} \rightarrow \mathbb{R}$ is assumed to treat the data points symmetrically.
    -   For example, OLS versus WLS.

## Split conformal prediction

(inductive conformal prediction)

1.  initial training data set: pre-trained model $\widehat{\mu}: \mathcal{X} \rightarrow \mathbb{R}$.

2.  holdout/calibration set: nonconformity scores $R_i=\left|Y_i-\widehat{\mu}\left(X_i\right)\right|, \quad i=1, \ldots, n$.

3.  prediction set: $\widehat{C}_n\left(X_{n+1}\right)=\widehat{\mu}\left(X_{n+1}\right) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \frac{1}{n+1} \cdot \delta_{R_i}+\frac{1}{n+1} \cdot \delta_{+\infty}\right)$.

    (the $\lceil(n+1)(1-\alpha)\rceil$th smallest of $R_1, \dots, R_n$)

Drawback: the loss of accuracy due to sample splitting.

## Full conformal prediction

(transductive conformal prediction)

1.  training data & a hypothesized test point: $\widehat{\mu}^y=\mathcal{A}\left(\left(X_1, Y_1\right), \ldots,\left(X_n, Y_n\right),\left(X_{n+1}, y\right)\right)$ for each $y \in \mathbb{R}$.
2.  residuals: $R_i^y= \begin{cases}\left|Y_i-\widehat{\mu}^y\left(X_i\right)\right|, & i=1, \ldots, n \\ \left|y-\widehat{\mu}^y\left(X_{n+1}\right)\right|, & i=n+1\end{cases}$.
3.  prediction set: $\widehat{C}_n\left(X_{n+1}\right)=\left\{y \in \mathbb{R}: R_{n+1}^y \leq \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^{n+1} \frac{1}{n+1} \cdot \delta_{R_i^y}\right)\right\}$.

Drawback: a steep computational cost.

*THEOREM:*

$\mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\} \geq 1-\alpha$ holds true for both split conformal and full conformal.

## Jackknife+

(close to cross-conformal prediction in @vovk2013, offering a compromise between the computational and statistical costs)

1.  training data with $i$th point removed: $\widehat{\mu}_{-i}=\mathcal{A}\left(\left(X_1, Y_1\right), \ldots,\left(X_{i-1}, Y_{i-1}\right),\left(X_{i+1}, Y_{i+1}\right), \ldots,\left(X_n, Y_n\right)\right)$.

2.  residuals: $R_i^{\mathrm{LOO}}=\left|Y_i-\widehat{\mu}_{-i}\left(X_i\right)\right|$.

3.  prediction set:

    ${\left[\mathrm{Q}_\alpha\left(\sum_{i=1}^n \frac{1}{n+1} \cdot \delta_{\widehat{\mu}_{-i}\left(X_{n+1}\right)-R_i^{\mathrm{LOO}}}+\frac{1}{n+1} \cdot \delta_{-\infty}\right),\right.} \left. \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \frac{1}{n+1} \cdot \delta_{\widehat{\mu}_{-i}\left(X_{n+1}\right)+R_i^{\mathrm{LOO}}}+\frac{1}{n+1} \cdot \delta_{+\infty}\right)\right]$

Drawback: while in practice the it generally provides coverage close to the target level $1−\alpha$, its theoretical guarantee only ensures $1−2\alpha$ probability of coverage in the worst case.

*THEOREM:*

$\mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\} \geq 1-2 \alpha$ holds true for jackknife+.

# Conformal time-series forecasting

@stankeviciute2021: CF-RNNs

**Multi-horizon time-series forecasting problem**

**Notation:**

-   the $i$th data point: $Z_i = (y_{1:t}^{(i)}, y_{t+1:t+H}^{(i)})$. Note that the label $y_{t+1:t+H}^{(i)}$ is now an $H$-dimensional value, in contrast with the scalar $y$ value from before.

**Assumption:**

-   exchangeable time-series observations

## Methodology

(Split conformal prediction)

1.  training set: train the underlying (auxiliary) model $\widehat{\mu}: \mathbb{R}^t \rightarrow \mathbb{R}^H$, which produces multi-horizon forecasts **directly** (conditionally independent predictions).

2.  calibration set: obtain the $H$-dimensional nonconformity scores

    $R_i=\left[\left|y_{t+1}^{(i)}-\hat{y}_{t+1}^{(i)}\right|, \ldots,\left|y_{t+H}^{(i)}-\hat{y}_{t+H}^{(i)}\right|\right]^{\top}$.

3.  prediction set: $\Gamma_1^\alpha\left(y_{(1: t)}^{(n+1)}\right), \ldots, \Gamma_H^\alpha\left(y_{(1: t)}^{(n+1)}\right)$, where $\Gamma_h^\alpha\left(y_{(1: t)}^{(n+1)}\right)=\left[\hat{y}_{t+h}^{(n+1)}-\hat{\varepsilon}_h, \hat{y}_{t+h}^{(n+1)}+\hat{\varepsilon}_h\right]$, $\forall h \in\{1, \ldots, H\}$ with the critical nonconformity scores $\hat{\varepsilon}_1, \ldots, \hat{\varepsilon}_H$ become the $\lceil(n+1)(1-\alpha / H)\rceil$-th smallest residuals in the corresponding nonconformity score distributions. (Bonferroni correction)

*THEOREM:*

-   $\mathcal{D}=\left\{\left(y_{1: t}^{(i)}, y_{t+1: t+H}^{(i)}\right)\right\}_{i=1}^n$: **exchangeable** time-series observations.
-   $\widehat{\mu}$: model predicting $H$-step forecasts using **the direct strategy**.

$$
\mathbb{P}\left(\forall h \in\{1, \ldots, H\} \cdot y_{t+h} \in\left[\hat{y}_{t+h}-\hat{\varepsilon}_h, \hat{y}_{t+h}+\hat{\varepsilon}_h\right]\right) \geq 1-\alpha .
$$

# NexCP: Conformal prediction beyond exchangeability

@barber2023

1.  Nonexchangeable conformal with a **symmetric** algorithm (weights)
2.  Nonexchangeable conformal with **nonsymmetric** algorithms (weights & swap)

## Notation

-   the $i$th data point $Z_i=(X_i,Y_i)$
-   the full data sequence $Z = (Z_1,\ldots,Z_{n+1})$
-   the sequence after swap $Z^i = (Z_{1},\ldots,Z_{i-1},Z_{n+1},Z_{i+1},Z_{n},Z_{i})$

## Methodology

1.  Choose **fixed** **(non-data-dependent)** weights $w_1,\ldots,w_n \in [0,1]$ with the intuition that a higher weight should be assigned to a data point that is "trusted" more.
2.  Normalize weights $\tilde{w}_i=\frac{w_i}{w_1+\cdots+w_n+1}, i=1, \ldots, n$ and $\tilde{w}_{n+1}=\frac{1}{w_1+\cdots+w_n+1}$.
3.  Generate "tagged" data points $(X_i, Y_i, t_i) \in \mathcal{X}\times\mathbb{R}\times\mathcal{T}$.
4.  Swap data set, resulting in $Z^K$ with $K \sim \sum_{i=1}^{n+1} \tilde{w}_i \cdot \delta_i$, i.e., two data points have swapped tags.
5.  Apply algorithm $\mathcal{A}$ to $Z^K$ in place of $Z$.

## Split conformal prediction

-   prediction set: $\widehat{C}_n\left(X_{n+1}\right)=\widehat{\mu}\left(X_{n+1}\right) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \tilde{w}_i \cdot \delta_{R_i}+\tilde{w}_{n+1} \cdot \delta_{+\infty}\right)$

## Full conformal prediction

1.  training data & a hypothesized test point: $\widehat{\mu}^{y, k}=\mathcal{A}\left(\left(X_{\pi_k(i)}, Y_{\pi_k(i)}^y, t_i\right): i \in[n+1]\right)$ for any $y \in \mathbb{R}$ and $k \in [n+1]$, where $\pi_k$ is the permutation on $[n+1]$ swapping indices $k$ and $n+1$, and $Y_i^y= \begin{cases}Y_i, & i=1, \ldots, n \\ y, & i=n+1 \end{cases}$.
2.  residuals: $R_i^{y, k}= \begin{cases}\left|Y_i-\widehat{\mu}^{y, k}\left(X_i\right)\right|, & i=1, \ldots, n \\ \left|y-\widehat{\mu}^{y, k}\left(X_{n+1}\right)\right|, & i=n+1 \end{cases}$.
3.  prediction set: $\widehat{C}_n\left(X_{n+1}\right)=\left\{y: R_{n+1}^{y, K} \leq Q_{1-\alpha}\left(\sum_{i=1}^{n+1} \tilde{w}_i \cdot \delta_{R_i^{y, K}}\right)\right\}$.

*THEOREM:*

-   Lower bounds on coverage.

    $$
    \mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\} \geq 1-\alpha-\sum_{i=1}^n \tilde{w}_i \cdot \mathrm{d}_{\mathrm{TV}}\left(R(Z), R\left(Z^i\right)\right)
    $$

-   Upper bounds on coverage.

    $$
    \mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\}<1-\alpha+\tilde{w}_{n+1}+\sum_{i=1}^n \tilde{w}_i \cdot \mathrm{d}_{\mathrm{TV}}\left(R(Z), R\left(Z^i\right)\right),
    $$ if $R_1^{Y_{n+1}, K}, \ldots, R_n^{Y_{n+1}, K}, R_{n+1}^{Y_{n+1}, K}$ are distinct with probability 1.

The results hold true for both nonexchangeable split conformal and full conformal.

So, if $\tilde{w}_{n+1}=\frac{1}{w_1+\cdots+w_n+1}$ is small (the effective sample size is large), then mild violations of exchangeability can only lead to mild undercoverage or to mild overcoverage.

## Jackknife+

1.  training data with $i$th point removed and data tag swapped:

    $\widehat{\mu}_{-i}^k=\mathcal{A}\left(\left(X_{\pi_k(j)}, Y_{\pi_k(j)}, t_j\right): j \in[n+1], \pi_k(j) \notin\{i, n+1\}\right)$.

2.  residuals: $R_i^{k, \mathrm{LOO}}=\left|Y_i-\widehat{\mu}_{-i}^k\left(X_i\right)\right|$.

3.  prediction set:

    ${\left[\mathrm{Q}_\alpha\left(\sum_{i=1}^n \tilde{w}_i \cdot \delta_{\widehat{\mu}_{-i}^K\left(X_{n+1}\right)-R_i^{K, \mathrm{LOO}}}+\tilde{w}_{n+1} \cdot \delta_{-\infty}\right),\right.} \left.\mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \tilde{w}_i \cdot \delta_{\widehat{\mu}_{-i}^K\left(X_{n+1}\right)+R_i^{K, \mathrm{LOO}}}+\tilde{w}_{n+1} \cdot \delta_{+\infty}\right)\right]$

*THEOREM:*

$\mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\} \geq 1-2 \alpha-\sum_{i=1}^n \tilde{w}_i \cdot \mathrm{d}_{\mathrm{TV}}\left(R_{\text {jack }+}(Z), R_{\text {jack }+}\left(Z^i\right)\right)$

# WCP: Conformal prediction under covariate shift

@tibshirani2019

A weighted version of conformal prediction, using a quantile of a suitably weighted empirical distribution of nonconformity scores.

## Setup/assumption - Covariate shift

Focus on settings in which the data $\left(X_i, Y_i\right), i=1, \ldots, n+1$ are no longer exchangeable. Specifically,

$$
\begin{aligned}
\left(X_i, Y_i\right) \stackrel{\text { i.i.d. }}{\sim} P&=P_X \times P_{Y \mid X}, \quad i=1, \ldots, n, \\
\left(X_{n+1}, Y_{n+1}\right) \sim \widetilde{P}&=\widetilde{P}_X \times P_{Y \mid X}, \text { independently. }
\end{aligned}
$$ {#eq-cs}

the test and training covariate distributions differ, but the likelihood ratio between the two distributions, $\mathrm{d} \widetilde{P}_X / \mathrm{d} P_X$, must be known exactly or well approximated for correct coverage.

## Methodology

**Prediction set:**

-   CP: $\widehat{C}_n\left(X_{n+1}\right)=\widehat{\mu}\left(X_{n+1}\right) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \frac{1}{n+1} \cdot \delta_{R_i}+\frac{1}{n+1} \cdot \delta_{+\infty}\right)$.
-   NexCP: $\widehat{C}_n\left(X_{n+1}\right)=\widehat{\mu}\left(X_{n+1}\right) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n \tilde{w}_i \cdot \delta_{R_i}+\tilde{w}_{n+1} \cdot \delta_{+\infty}\right)$.
    -   weights $w$ are fixed

    -   $\tilde{w}_i=\frac{w_i}{w_1+\cdots+w_n+1}, i=1, \ldots, n$ and $\tilde{w}_{n+1}=\frac{1}{w_1+\cdots+w_n+1}$
-   WCP: $\widehat{C}_n\left(X_{n+1}\right)=\widehat{\mu}\left(X_{n+1}\right) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{i=1}^n p_i^w(x) \delta_{R_i}+p_{n+1}^w(x) \delta_{\infty}\right)$.
    -   $w = \mathrm{d} \widetilde{P}_X / \mathrm{d} P_X$ or $w \propto \mathrm{d} \widetilde{P}_X / \mathrm{d} P_X$
    -   $p_i^w(x)=\frac{w\left(X_i\right)}{\sum_{j=1}^n w\left(X_j\right)+w(x)}, i=1, \ldots, n,$ and $p_{n+1}^w(x)=\frac{w(x)}{\sum_{j=1}^n w\left(X_j\right)+w(x)}$

The **weight function** $\hat{w}$ can be estimated using logistic regression, random forests, etc.

*THEOREM:*

Assume data from the model @eq-cs. Assume $\widetilde{P}_X$ is absolutely continuous with respect to $P_X$, and denote $w = \mathrm{d} \widetilde{P}_X / \mathrm{d} P_X$. For any score function $S$, and any $\alpha \in (0,1)$, define for $x \in \mathbb{R}^d$. Then

$$
\mathbb{P}\left\{Y_{n+1} \in \widehat{C}_n\left(X_{n+1}\right)\right\} \geq 1-\alpha.
$$

## Comparison to NexCP

1.  Assumption
    -   WCP: covariate shift
    -   NexCP: do not make any assumption on the joint distribution of the $n + 1$ points
2.  Weights
    -   WCP: a function of the data point $(X_i,Y_i)$ to compensate for the **known** distribution shift.
    -   NexCP: required to be fixed, can compensate for **unknown** violations of the exchangeability assumption, as long as the violations are **small** (to ensure a low coverage gap).
3.  Nonsymmetric algorithm
    -   WCP: No.
    -   NexCP: Yes.
4.  For exchangeable data
    -   WCP: does not have any coverage guarantee.
    -   NexCP: retains exact coverage.

# ACP: Adaptive conformal inference under distribution shift

@gibbs2021

-   No assumptions on the data-generating distribution.

-   Modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated.

-   Adjust significance level $\alpha$ based on rolling coverage of $Y_t$ .

## Methodology

Work with score function $S(\cdot)$ and quantile function $\hat{Q}(\cdot)$.

**Some facts:**

-   If the distribution of the data is shifting over time, both functions should be regularly re-estimated to align with the most recent observations.

-   The realized miscoverage rate $M_t(\alpha)$ also varies over time and may not be equal or close to $\alpha$.

**Assumptions:**

Assume that there may be an alternative value $\alpha^{*} \in [0,1]$ such that $M_t(\alpha^*)\cong\alpha$.

Assume that with probability one, $\hat{Q}_t(\cdot)$ is continuous, non-decreasing and such that $\hat{Q}_t(0)=-\infty$ and $\hat{Q}_t(1)=\infty$.

**Adaptive conformal inference:**

-   Under assumptions, $M_t(\cdot)$ will be non-decreasing on $[0,1]$ with $M_t(0)=0$ and $M_t(1)=1$.

-   Define $\alpha_t^*:=\sup \left\{\beta \in[0,1]: M_t(\beta) \leq \alpha\right\}$, then $M_t\left(\alpha_t^*\right)=\alpha$.

-   Use a simple **update process** to perform the calibration.

    -   Intuition: after examining the empirical miscoverage frequency of the previous prediction sets, decreasing (increasing) estimate of $\alpha_t^*$ if the prediction sets were historically under-covering (over-covering) $Y_t$.

    -   Let $\alpha_1=\alpha$, consider the **update**

        $$
        \alpha_{t+1}:=\alpha_t+\gamma\left(\alpha-\operatorname{err}_t\right) 
        $$

        OR

        $$
        \alpha_{t+1}=\alpha_t+\gamma\left(\alpha-\sum_{s=1}^t w_s \operatorname{err}_s\right),
        $$

        where $\gamma>0$ is a fixed step size parameter whose choice gives a tradeoff between adaptability and stability.

        So the distribution is allowed to shift continuously over time.

*THEOREM:*

With probability one we have that for all $T \in \mathbb{N}$,

$$
\left|\frac{1}{T} \sum_{t=1}^T e r r_t-\alpha\right| \leq \frac{\max \left\{\alpha_1, 1-\alpha_1\right\}+\gamma}{T \gamma} .
$$

In particular, $\lim _{T \rightarrow \infty} \frac{1}{T} \sum_{t=1}^T err_t \stackrel{\text { a.s. }}{=} \alpha$.

# PID: Conformal PID control for time series prediction

@angelopoulos2023

-   PID: proportional-integral-derivative (control theory).

-   reactive + forward-looking.

## Methodology

The proposed conformal PID controller is given by

$$
q_{t+1}=\underbrace{\eta (\operatorname{err}_t-\alpha)}_{\mathrm{P}}+\underbrace{r_t\left(\sum_{i=1}^t (\operatorname{err}_i-\alpha)\right)}_{\mathrm{I}}+\underbrace{g_t^{\prime}}_{\mathrm{D}}.
$$

The goal is to achieve **long-run coverage** in time, i.e., $\frac{1}{T} \sum_{t=1}^T \operatorname{err}_t=\alpha+o(1)$, where $o(1)$ denotes a quantity that tends to zero as $T \rightarrow \infty$.

-   P: Quantile tracking. ACI can be regarded as a special case but it can sometimes output infinite or null prediction sets, while quantile tracking on the scale of the original score sequence does not have this behavior.

-   I: Error integration. $r_t$ should be a saturation function to achieve long-run coverage.

-   D: Scorecasting (forward-looking). It trains a second model to predict the quantile of the next score.

Reparametrize the PID formula to produce a sequence of quantile estimates $q_t$, $t \in N$ used in the prediction sets:

let $\hat{q}_{t+1}$ be any function of the past: $x_i, y_i, q_i$, for $i \leq t$, then update

$$
q_{t+1}=\underbrace{\hat{q}_{t+1}}_{\text{scorecaster}}+\underbrace{r_t\left(\sum_{i=1}^t\left(\operatorname{err}_i-\alpha\right)\right)}_{\text{integrator}},
$$

which is generally useful and is the main focus of the paper.

The applications only consider one-step-ahead forecast.

# Others

## EnbPI: predictive inference method around ensemble estimators

@xu2021

-   For Dynamic time series

-   Ensemble point forecasts + update residuals

## SPCI: Sequential Predictive Conformal Inference

@xu2023

-   Adaptively re-estimate the conditional quantile of non-conformity scores, upon leveraging the temporal dependency among residuals. Random Forest for quantile regression is used.

## Kath & Ziel (2021, IJF)

@kath2021

**Method 1.**

The steps are:

1.  Re-arrange time series data as form $Z=\{(\boldsymbol{x}_1,y_1), \ldots, (\boldsymbol{x}_L,y_L)\}$.
2.  Randomly split observations into subsets $\pi$ (training set) and $1-\pi$ (calibration set).
3.  Train a random forecast model using the training set and generate out-of-sample forecasts.
4.  Calculate non-conformity score.
5.  Obtain final prediction interval estimation.

**Method 2.**

The differences compared to Method 1 are:

1.  Here an adjusted normalized non-conformity score $\lambda_{i, h}=\frac{\left|y_{i, h}-\hat{y}_{i, h}\right|}{\left|\hat{\varepsilon}_{i, h}\right|}$ is used.
2.  A second explicit error estimation model is trained to estimate the error associated with the prediction $\hat{y}_{t,h}$, i.e., $\varepsilon_{t,h}$.
3.  The new interval forecast is now given by $y_{\alpha, T+1, h}=\hat{y}_{T+1, h} \pm \lambda_{L+1, h}^\alpha\left|\hat{\varepsilon}_{L+1, h}\right|$.

The applications only consider one-step-ahead forecast.

## TimeGPT

@garza2023

For uncertainty quantification, they perform rolling forecasts on the latest available data to estimate the model's errors in forecasting the particular target time series.

The $h$-step-ahead forecast is generated based on $h$-step-ahead non-conformity errors on the calibration set separately.

## LCP: Localized conformal prediction

@guan2022

The **weight** on data point $i$ is determined as **a function of the distance** $\|X_i - X_{n+1}\|_2$, to enable predictive coverage that holds **locally** (in neighborhoods of $X$ space, that is, an approximation of prediction that holds conditional on the value of $X_{n+1}$).

# Simulation

## Setup

Simulate a time series $y$ with length $T=5000$ from an AR(2) model with $\phi_1 = 0.8$, $\phi_2=-0.5$, and $\sigma^2 = 1$.

Only consider one-step-ahead forecasting, i.e., $h=1$.

```{r}
#| label: fig-ar2
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 7
#| fig-height: 3
#| fig-cap: Simulated time series from an AR(2) model.

load("../data/AR2data.rda")
autoplot(series) +
  ggtitle("AR(2) process") +
  xlab("Time") +
  ylab("") +
  theme_bw()
```

## Split conformal prediction with fixed training set

### Details

Let $n$ be the number of data used to fit an AR(2) model, and $m$ be the number of data in the calibration set.

-   Step 1. Rearrange the time series as $Z_i = (X_i, Y_i)$, where $X_i = y_{i:(i+1)}$ and $Y_i=y_{i+2}$, $i=1,2,\ldots,T-2$.

-   Step 2. Train an AR(2) model $\hat{\mu}$ based on training set with length $n$, $Z_{tr} = (Z_1,Z_2,\ldots,Z_{n})$.

-   Step 3. Calculate nonconformity scores (absolute residuals) based on calibration set with length $m$, i.e., $R_i = |Y_i - \hat{\mu}(X_i)|$, $i=n+1,n+2,\ldots,n+m$.

-   Step 4. Generate PI on test set. $\hat{C_{m}}(X_{i}) = \hat{\mu}(X_{i}) \pm \mathrm{Q}_{1-\alpha}\left(\sum_{j=n+1}^{n+m} w_j \cdot \delta_{R_j}+ w_{i} \cdot \delta_{+\infty}\right)$ for $i=n+m+i,\ldots,T-2$.

### Results

Let $n=m=500$ and fit the AR(2) model using the `lm` function.

Consider methods: CP, WCP.LR, WCP.RF, NexCP with $\alpha=0.1$.

```{r}
#| label: fig-ar2fix
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 7
#| fig-height: 10
#| fig-cap: Local coverage frequencies and width for split conformal prediction methods with fixed training set (k=500).

load("../data/AR2_split_plot.rda")
ggarrange(lcl.split, mw.split, mdw.split,
          ncol = 1, nrow = 3,
          common.legend = TRUE, legend = "bottom")
```

Issues:

-   PIs have constant width over the test set. We can generate PIs with varying local width by using a function to perform training on the absolute residuals i.e., to produce an estimator of $\operatorname{E}(R|X)$.

-   When generating weights via GLM or RF in WCP method, we need to use all data from the test set, which is not reasonable.

-   Regression-based model.

## Split conformal prediction with rolling training set

### Details

Let $n$ be the number of observations used to fit an AR(2) model, and $m$ be the number of observations in the calibration set.

For $i=n+1,n+2,…,T$:

-   Step 1. Fit an AR(2) model $\hat{\mu}_{i-1}$ based on observations $y_{(i-n):(i-1)}$, generate one-step-ahead forecast $\hat{y}_{i}$.

-   When $i > n+m$:

    -   Step 2. Calculate weights for the updated calibration set with length $m$ and the updated test set with length 1 using different methods. Here WCP can not be applied because we only have a test set with length equal to one.

    -   Step 3. Generate PI on test set. $\hat{y}_i \pm \mathrm{Q}_{1-\alpha}\left(\sum_{j=i-m}^{i-1} w_i \cdot \delta_{R_j}+ w_{i} \cdot \delta_{+\infty}\right)$.

    -   Setp 4 for ACP. Update $\alpha$ based on the recent empirical miscoverage frequency.

-   Step 5. Calculate nonconformity scores (absolute residuals) $R_i = |y_i - \hat{y}_{i}|$.

### Results

Let $n=m=500$ and fit AR(2) models using the `Arima` function with setting `order = c(2,0,0), include.mean = TRUE, method = "CSS"` to make it comparable with the previous result.

Consider methods: AR, CP, NexCP, ACP, PID with $\alpha=0.1$.

For the PID method:

-   The AR(2) models are trained using `nfit` observations through a rolling window strategy, as with the other methods.
-   Theta method is used as the scorecaster to forecast the conformity scores. An expanding window strategy is used if `ncast = NULL`; a rolling window strategy is used if `ncast != NULL` (here we use rolling windows with `ncast = ncal` for comparison with other methods).
-   The first `nburnin` outputs are removed to have output of equal length. (`nburnin` is equal to `ncal` in other methods.)
-   Some parameters.
    -   Constants Csat and KI for the integrator $r_t(x)=K_{\mathrm{I}} \tan \left(x \log (t) /\left(t C_{\mathrm{sat}}\right)\right)$ are determined by some heuristics. See appendix B for more details. Here, $T=5000$, $\delta=0.01$, and $K_{\mathrm{I}}=2$.
    -   The learning rate for the quantile tracking (P): $\eta=0.1\hat{B}_t$.

```{r}
#| label: fig-ar2roll
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 7
#| fig-height: 10
#| fig-cap: Local coverage frequencies and width for split conformal prediction methods with rolling training set and calibration set (k=500).

load("../data/AR2_update_plot.rda")
ggarrange(lcl.update, mw.update, mdw.update,
          ncol = 1, nrow = 3,
          common.legend = TRUE, legend = "bottom")
```

Features:

-   The fitted model is updating over time.
-   PIs have changing width over the test set.
-   Other non-regression-based models can be considered.
-   Time-consuming because of the rolling window approach (but is still very quick for this simulation).

### Analysis on PID

**The effect of integration (Csat and KI)**

$C_{\mathrm{sat}}$ controls the guaranteed coverage.

$K_{\mathrm{I}}$ controls the change scale of the integrator.

1.  Wrong setting: $C_{\mathrm{sat}}=1$, $K_{\mathrm{I}}=200$
2.  Heuristics setting: $C_{\mathrm{sat}}=0.5618745$, $K_{\mathrm{I}}=2$ used in the Results section.

```{r}
#| label: fig-ar2roll-wrongPIDsetting
#| echo: false
#| message: false
#| warning: false
#| results: hide
#| fig-width: 7
#| fig-height: 10
#| fig-cap: (Wrong setting.) Local coverage frequencies and width for split conformal prediction methods with rolling training set and calibration set (k=500).

load("../data/AR2_update_plot_wrongPIDsetting.rda")
ggarrange(lcl.update, mw.update, mdw.update,
          ncol = 1, nrow = 3,
          common.legend = TRUE, legend = "bottom")
```

```{r}
#| label: fig-ar2roll-wrongPIDsetting-boxplot-01
#| echo: false
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 3
#| fig-cap: Boxplots for the width of various methods. For ACP, the learning rate is set to 0.005. For P, PI, and PID, the learning rate is set to 0.1B.

load("../data/AR2_update_wrongPIDsetting.rda")
PID_wrong_len <- data.frame(Method = "WPID", Width = len.update$PID)
load("../data/AR2_update.rda")
ACP_len <- data.frame(Method = "ACP", Width = len.update$ACP)
load("../data/AR2_PID_01.rda")
PID_len <- data.frame(Method = "PID", Width = len.PID$PID)
PI_len <- data.frame(Method = "PI", Width = len.PID$PI)
P_len <- data.frame(Method = "P", Width = len.PID$P)
len <- rbind(ACP_len, PID_len, PI_len, P_len, PID_wrong_len)
ggplot(len, aes(x = Method, y = Width)) +
  geom_boxplot() +
  theme_bw()
```

```{r}
#| label: fig-ar2roll-wrongPIDsetting-boxplot-001
#| echo: false
#| message: false
#| warning: false
#| fig-width: 7
#| fig-height: 3
#| fig-cap: Boxplots for the width of various methods. For ACP, the learning rate is still set to 0.005. But, for P, PI, and PID, the learning rate is set to 0.01B.

load("../data/AR2_update.rda")
ACP_len <- data.frame(Method = "ACP", Width = len.update$ACP)
load("../data/AR2_PID_001.rda")
PID_len <- data.frame(Method = "PID", Width = len.PID$PID)
PI_len <- data.frame(Method = "PI", Width = len.PID$PI)
P_len <- data.frame(Method = "P", Width = len.PID$P)
len <- rbind(ACP_len, PID_len, PI_len, P_len)
ggplot(len, aes(x = Method, y = Width)) +
  geom_boxplot() +
  theme_bw()
```

**The effect of scorecasting** (more variance introduced if there is no pattern in the score sequence)

# To do list

-   [ ] Generalize the existing methods and get a general function for implementation.
    -   [ ] Symmetric/asymmetric scores.
    -   [ ] Expanding/rolling window strategy.
    -   [ ] Include weights in PID.
    -   [ ] Handle seasonal time series and specify methods used for forecasting and scorecasting.
-   [ ] Multi-step ahead forecasting.
    -   [ ] Correlation between multi step ahead residuals.
    -   [ ] Similar strategy as PID.
-   [ ] More simulations and real-world data.
-   [ ] Theoretical proof.
-   [ ] Other data-dependent weight.
-   [ ] Hierarchical time series?

# References

::: {#refs}
:::
